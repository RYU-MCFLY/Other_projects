{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RMT_Sim_Plots.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RYU-MCFLY/RYU-MCFLY/blob/main/RMT_Sim_Plots.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfYAcVEqG-rh"
      },
      "source": [
        "# Random Matrix Theory Simulations with Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b_iIGMBHZW5"
      },
      "source": [
        "Within this Jupyter notebook, we analyze the eigenvalue structure of the Hessians of a single-ReLU layer neural network throughout gradient descent and how this relates to its loss. For quick results, run \"Imports\" and \"Key Function Definitions\" as sections before plotting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7qrrbGwI7Bl"
      },
      "source": [
        "We set up our neural network using the following parameters:\n",
        "* $n_0$ represents the number of parameters in the input layer. This is customizable.\n",
        "* $n_1$ represents the size of the hidden ReLU layer. This is customizable.\n",
        "* $n_2$ represents the size of the output layer. This is customizable.\n",
        "* $m$ represents the number of data points passed into the neural network. This is customizable.\n",
        "* $W^{(1)}$ represents the first weight matrix in the network. It is a $n_1$ x $n_0$ matrix with each entry being Gaussian with mean 0 and variance $\\sigma^2$ at initialization. \n",
        "* $W^{(2)}$ represents the second weight matrix in the network. It is a $n_2$ x $n_1$ matrix with each entry being Gaussian with mean 0 and variance $\\sigma^2$ at initialization. \n",
        "* $x$ represents our input data. It is an $n_0$ x $m$ matrix with Gaussian entries with mean 0 and variance $\\sigma^2$.\n",
        "* $y$ represents our target data. It is an $n_2$ x $m$ matrix with Gaussian entries with mean 0 and variance $\\sigma^2$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgKKgMHeMDKa"
      },
      "source": [
        "To generate predictions given $W^{(1)}$ and $W^{(2)}$ at some time t, we define $z = W_1x$ and $[A_{xy}]_+ = \\text{max}(A_{xy}, 0)$ as our ReLU function, where $A$ is any matrix. This gives us the following output, $\\hat{y}$:\n",
        "\\begin{equation}\n",
        "  \\hat{y}_{i\\mu} =  \\sum_{k=1}^{n_1} W^{(2)}_{ik} [z_{k\\mu}]_+\n",
        "\\end{equation}\n",
        "Using $\\hat{y}$, we define $e = \\hat{y} - y$ and continue by defining our loss function, $\\mathcal{L}$, as the mean squared error using $e$:\n",
        "\\begin{equation}\n",
        "  \\mathcal{L} = \\frac{1}{2m} \\|e\\|^2 =  \\frac{1}{2m} \\sum_{i, \\mu=1}^{n_2, m} e_{iu}^2 \n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvp0i6k4TbV6"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAgzpeLVKx66"
      },
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy.linalg as npla\n",
        "from scipy.optimize import curve_fit\n",
        "from ipywidgets import widgets, interactive\n",
        "from IPython.utils import io"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUgi7TIHTfmg"
      },
      "source": [
        "# Key Function Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpvLUcpVO3ma"
      },
      "source": [
        "## Gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sG2m4Hk8_0l"
      },
      "source": [
        "Because our network depends on two weight matrices, the gradients of our loss function are:\n",
        "\\begin{equation}\n",
        "  \\nabla (\\mathcal{L})_{abcd} = (\\frac{\\partial \\mathcal{L}}{\\partial W^{(1)}_{ab}}, \\frac{\\partial \\mathcal{L}}{\\partial W^{(2)}_{cd}}) = (\\frac{\\partial  \\frac{1}{2m} \\sum_{i, \\mu = 1}^{n_2, m} e_{i\\mu}^2}{\\partial W^{(1)}_{ab}}, \\frac{\\partial \\frac{1}{2m} \\sum_{i, \\mu = 1}^{n_2, m} e_{i\\mu}^2}{\\partial W^{(2)}_{cd}}) = \\frac{1}{m} \\sum_{i, \\mu = 1}^{n_2, m} e_{i \\mu} (\\frac{\\partial e_{i\\mu}}{\\partial W^{(1)}_{ab}}, \\frac{\\partial e_{i\\mu}}{\\partial W^{(2)}_{cd}})\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfaQlVfOPyRP"
      },
      "source": [
        "To calculate our gradients, we introduce the Jacobians $J^{(1)}$ and $J^{(2)}$ over $W^{(1)}$ and $W^{(2)}$, respectively.\n",
        "\n",
        "\\begin{equation}\n",
        "  J^{(1)}_{iab\\mu} = \\frac{\\partial e_{i\\mu}}{\\partial W^{(1)}_{ab}} = \\frac{\\partial \\hat{y}_{i\\mu}}{\\partial W^{(1)}_{ab}} =\n",
        "  \\begin{cases}\n",
        "    W^{(2)}_{ia}x_{b\\mu} & \\text{if } \\sum_{j=1}^{n_0} W^{(1)}_{aj}x_{j\\mu} > 0 \n",
        "    \\\\ 0 & \\text{otherwise}\n",
        "  \\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "  J^{(2)}_{icd\\mu} = \\frac{\\partial e_{i\\mu}}{\\partial W^{(2)}_{cd}} = \\frac{\\partial \\hat{y}_{i\\mu}}{\\partial W^{(1)}_{ab}} =\n",
        "  \\begin{cases}\n",
        "    [z^{(1)}_{d\\mu}]_+ & \\text{if } i = c \n",
        "    \\\\ 0 & \\text{otherwise}\n",
        "  \\end{cases}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez9qWwG5bk4V"
      },
      "source": [
        "This leaves us with the gradients of our loss as:\n",
        "\\begin{equation}\n",
        "  \\nabla (\\mathcal{L})_{abcd} = \\frac{1}{m} \\sum_{i, \\mu = 1}^{n_2, m} e_{i \\mu} (J^{(1)}_{iab\\mu}, J^{(2)}_{icd\\mu})\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dXYStWPLPi3"
      },
      "source": [
        "def gradient(x, W_2, e, prop):\n",
        "  (n2, n1) = W_2.shape\n",
        "  (n0, m) = x.shape\n",
        "\n",
        "  J_1 = np.tensordot(W_2, x, axes=0)\n",
        "  for a, u in itertools.product(range(n1), range(m)):\n",
        "    if prop[a, u] == 0:\n",
        "      J_1[0:n2, a, 0:n0, u] = 0\n",
        "\n",
        "  J_2 = np.tensordot(np.ones((n2, n2)), prop, axes=0)\n",
        "  \n",
        "  A=np.einsum('iu,iabu->ab',e,J_1)/m\n",
        "  B=np.einsum('iu,icdu->cd',e,J_2)/m\n",
        "\n",
        "  return A, B"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFjfjCu0Z3ta"
      },
      "source": [
        "## Hessians"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBI9TTMfYYN9"
      },
      "source": [
        "From the gradients of our loss functions, we find its Hessians:\n",
        "\\begin{equation}\n",
        "H \\mathcal{L} = \n",
        "  \\begin{bmatrix}\n",
        "    \\frac{\\partial^2 \\mathcal{L}}{\\partial W^{(1)}_{ab} \\partial W^{(1)}_{a'b'}} & \n",
        "    \\frac{\\partial^2 \\mathcal{L}}{\\partial W^{(1)}_{ab} \\partial W^{(2)}_{cd}} \n",
        "    \\\\\n",
        "    \\frac{\\partial^2 \\mathcal{L}}{\\partial W^{(2)}_{c'd'} \\partial W^{(1)}_{a'b'}} \n",
        "    & \\frac{\\partial^2 \\mathcal{L}}{\\partial W^{(2)}_{c'd'} \\partial W^{(2)}_{cd}}\n",
        "  \\end{bmatrix}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSGe0HMzYIPa"
      },
      "source": [
        "We can simplify these Hessians using the chain rule to be:\n",
        "\\begin{equation}\n",
        "H \\mathcal{L} = \\sum_{i, \\mu = 1}^{n_2, m}\n",
        "  \\begin{bmatrix}\n",
        "    \\frac{\\partial e_{i\\mu}}{\\partial W^{(1)}_{ab}} \\frac{\\partial e_{i\\mu}}{\\partial W^{(1)}_{a'b'}} + e_{i \\mu} \\frac{\\partial^2 e_{i \\mu}}{\\partial W^{(1)}_{ab} \\partial W^{(1)}_{a'b'}} \n",
        "    & \n",
        "    \\frac{\\partial e_{i\\mu}}{\\partial W^{(1)}_{ab}} \\frac{\\partial e_{i\\mu}}{\\partial W^{(2)}_{cd}} + e_{i \\mu} \\frac{\\partial^2 e_{i \\mu}}{\\partial W^{(1)}_{ab} \\partial W^{(2)}_{cd}} \n",
        "    \\\\\n",
        "    \\frac{\\partial e_{i\\mu}}{\\partial W^{(2)}_{c'd'}} \\frac{\\partial e_{i\\mu}}{\\partial W^{(1)}_{a'b'}} + e_{i \\mu} \\frac{\\partial^2 e_{i \\mu}}{\\partial W^{(2)}_{c'd'} \\partial W^{(1)}_{a'b'}} \n",
        "    & \n",
        "    \\frac{\\partial e_{i\\mu}}{\\partial W^{(2)}_{c'd'}} \\frac{\\partial e_{i\\mu}}{\\partial W^{(2)}_{cd}} + e_{i \\mu} \\frac{\\partial^2 e_{i \\mu}}{\\partial W^{(2)}_{c'd'} \\partial W^{(2)}_{cd}} \n",
        "  \\end{bmatrix}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLLifvYKP1f1"
      },
      "source": [
        "We express $H$ as the sum of two matrices $H_0$ and $H_1$ defined as:\n",
        "\n",
        "\\begin{equation}\n",
        "H_0 = \\sum_{i, \\mu = 1}^{n_2, m}\n",
        "  \\begin{bmatrix}\n",
        "    \\frac{\\partial e_{i\\mu}}{\\partial W^{(1)}_{ab}} \\frac{\\partial e_{i\\mu}}{\\partial W^{(1)}_{a'b'}} \n",
        "    & \n",
        "    \\frac{\\partial e_{i\\mu}}{\\partial W^{(1)}_{ab}} \\frac{\\partial e_{i\\mu}}{\\partial W^{(2)}_{cd}} \n",
        "    \\\\\n",
        "    \\frac{\\partial e_{i\\mu}}{\\partial W^{(2)}_{c'd'}} \\frac{\\partial e_{i\\mu}}{\\partial W^{(1)}_{a'b'}}\n",
        "    & \n",
        "    \\frac{\\partial e_{i\\mu}}{\\partial W^{(2)}_{c'd'}} \\frac{\\partial e_{i\\mu}}{\\partial W^{(1)}_{cd}}\n",
        "  \\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "H_1 = \\sum_{i, \\mu = 1}^{n_2, m}\n",
        "  \\begin{bmatrix}\n",
        "    e_{i \\mu} \\frac{\\partial^2 e_{i \\mu}}{\\partial W^{(1)}_{ab} \\partial W^{(1)}_{a'b'}} \n",
        "    & \n",
        "    e_{i \\mu} \\frac{\\partial^2 e_{i \\mu}}{\\partial W^{(1)}_{ab} \\partial W^{(2)}_{cd}} \n",
        "    \\\\\n",
        "    e_{i \\mu} \\frac{\\partial^2 e_{i \\mu}}{\\partial W^{(2)}_{c'd'} \\partial W^{(1)}_{a'b'}} \n",
        "    & \n",
        "    e_{i \\mu} \\frac{\\partial^2 e_{i \\mu}}{\\partial W^{(2)}_{c'd'} \\partial W^{(2)}_{cd}} \n",
        "  \\end{bmatrix}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3ecyiXyO2y9"
      },
      "source": [
        "We can calculate $H_0$ using the Jacobians $J^{(1)}$ and $J^{(2)}$ introduced above, to give:\n",
        "\\begin{equation}\n",
        "H_0 = \\sum_{i, \\mu = 1}^{n_2, m}\n",
        "  \\begin{bmatrix}\n",
        "    J^{(1)}_{iab\\mu} J^{(1)T}_{\\mu b'a'i}\n",
        "    & \n",
        "    J^{(1)}_{iab\\mu} J^{(2)T}_{\\mu dci}\n",
        "    \\\\\n",
        "    J^{(2)}_{ic'd'\\mu} J^{(1)T}_{\\mu b'a'i}\n",
        "    & \n",
        "    J^{(2)}_{ic'd'\\mu} J^{(2)T}_{\\mu dci}\n",
        "  \\end{bmatrix}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ayw59yP-ULc_"
      },
      "source": [
        "In calculating $H_1$, we determine, based on the linearity of our neural network in each weight matrix, that only our mixed derivatives are non-zero and can calculate them, and consequently $H_1$, as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\frac{\\partial^2 \\hat{y}_{i\\mu}}{\\partial W_{ab}^{(1)} \\partial W_{cd}^{(2)}} =\n",
        "  \\begin{cases}\n",
        "    x_{b\\mu} & \\text{if } \\sum_{j=1}^{n_0} W^{(1)}_{aj}x_{j\\mu} > 0 \\text{ and } a = d \n",
        "    \\\\ 0 & \\text{otherwise}\n",
        "  \\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "  H_1 = \\sum_{i, \\mu = 1}^{n_2, m}\n",
        "  \\begin{bmatrix}\n",
        "    0 & \n",
        "    e_{i\\mu} \\frac{\\partial^2 \\hat{y}_{i\\mu}}{\\partial W_{ab}^{(1)} \\partial W_{cd}^{(2)}}\n",
        "    \\\\ (e_{i\\mu} \\frac{\\partial^2 \\hat{y}_{i\\mu}}{\\partial W_{ab}^{(1)} \\partial W_{cd}^{(2)}})^T \n",
        "    & 0\n",
        "  \\end{bmatrix}\n",
        "\\end{equation}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6_AZxvvMcc9"
      },
      "source": [
        "def hessian(W1, W2, x, y, l, l0):\n",
        "  (n2, n1) = W2.shape\n",
        "  (n0, m) = x.shape\n",
        "\n",
        "  z = W1 @ x\n",
        "  prop = np.maximum(z, np.zeros_like(z))\n",
        "  y_hat = W2 @ prop\n",
        "  e = y_hat - y \n",
        "\n",
        "  def compute_H0():\n",
        "    J_1 = np.tensordot(W2, x, axes=0)\n",
        "    for a, u in itertools.product(range(n1), range(m)):\n",
        "      if prop[a, u] == 0:\n",
        "        J_1[0:n2, a, 0:n0, u] = 0\n",
        "\n",
        "    J_2 = np.tensordot(np.ones((n2, n2)), prop, axes=0)\n",
        "\n",
        "    H_0_tl = np.einsum('xaby,xcdy->abcd', J_1, J_1)\n",
        "    H_0_tl_flat = H_0_tl.reshape((n1 * n0, n1 * n0))\n",
        "\n",
        "    H_0_tr = np.einsum('xaby,xcdy->abcd', J_1, J_2)\n",
        "    H_0_tr_flat = H_0_tr.reshape((n1 * n0, n1 * n2))\n",
        "\n",
        "    H_0_bl = np.einsum('xcdy,xaby->cdab', J_2, J_1)\n",
        "    H_0_bl_flat = H_0_bl.reshape((n1 * n2, n1 * n0))\n",
        "\n",
        "    H_0_br = np.einsum('xaby,xcdy->abcd', J_2, J_2)\n",
        "    H_0_br_flat = H_0_br.reshape((n1 * n2, n1 * n2))\n",
        "\n",
        "    H_0 = np.block([[H_0_tl_flat, H_0_tr_flat], [H_0_bl_flat, H_0_br_flat]])\n",
        "    return H_0\n",
        "\n",
        "  def compute_H1():\n",
        "    H_1_tl = np.zeros((n1 * n0, n1 * n0))\n",
        "\n",
        "    H_1_tr = np.zeros((n0 * n1, n1 * n2))\n",
        "    for q, p in itertools.product(range(n0 * n1), range(n1 * n2)):\n",
        "      a = int(q / n0)\n",
        "      b = q % n0\n",
        "      c = int(p / n1)\n",
        "      d = p % n1\n",
        "\n",
        "      if a == d:\n",
        "        e_masked = np.where(prop[a] > 0, e[c], 0)\n",
        "        H_1_tr[q][p] = e_masked @ x[b]\n",
        "\n",
        "    H_1_br = np.zeros((n1 * n2, n1 * n2))\n",
        "\n",
        "    H_1_bl = H_1_tr.transpose()\n",
        "\n",
        "    H_1 = np.block([[H_1_tl, H_1_tr], [H_1_bl, H_1_br]])\n",
        "    return H_1 / m * np.sqrt(l / l0)\n",
        "\n",
        "  H0, H1 = compute_H0(), compute_H1()\n",
        "  H = H0 + H1\n",
        "\n",
        "  return H0, H1, H"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOhj7H0EZ7WC"
      },
      "source": [
        "## Simulation Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCnq5ldILjTb"
      },
      "source": [
        "def gradient_descent(W1, W2, x, y, num_iters, step, prop, e, id):\n",
        "  (n2, n1) = W2.shape\n",
        "  (n0, m) = x.shape\n",
        "\n",
        "  losses = np.empty(0)\n",
        "  for j in range(num_iters):\n",
        "    G1, G2 = gradient(x, W2, e, prop)\n",
        "    W1 = W1 - G1 * step\n",
        "    W2 = W2 - G2 * step\n",
        "    \n",
        "    z = W1 @ x\n",
        "    prop = np.maximum(z, np.zeros_like(z))\n",
        "    y_hat = W2 @ prop\n",
        "    e = y_hat - y\n",
        "\n",
        "    loss = npla.norm(e) ** 2 / (2 * m)\n",
        "    losses = np.append(losses, loss)\n",
        "    \n",
        "  return W1, W2, losses, e, prop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a85GIv5XPQky"
      },
      "source": [
        "def run_gd(n0, n1, n2, m, sigma, num_iters):\n",
        "  W1 = np.random.randn(n1, n0) * sigma\n",
        "  W2 = np.random.randn(n2, n1) * sigma\n",
        "\n",
        "  x = np.random.randn(n0, m) * sigma\n",
        "  y = np.random.randn(n2, m) * sigma\n",
        "\n",
        "  z = W1 @ x\n",
        "  prop = np.maximum(z, np.zeros_like(z))\n",
        "  y_hat = W2 @ prop\n",
        "  e = y_hat - y\n",
        "\n",
        "  step = 0.0005\n",
        "\n",
        "  W1_gd, W2_gd, losses, e_gd, prop_gd = gradient_descent(W1, W2, x, y, num_iters, step, prop, e,1)\n",
        "\n",
        "  return losses, W1_gd, W2_gd, x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVbB6bRlJXKv"
      },
      "source": [
        "def run_sim(n0, n1, n2, m, sigma, num_runs):\n",
        "  num_iters = 5000\n",
        "  H0_eigs_arr, H1_eigs_arr, H_eigs_arr = np.empty(0), np.empty(0), np.empty(0)\n",
        "  phi = ((n0 + n2) * n1) / (n2 * m)\n",
        "      \n",
        "  loss_array = np.empty((num_runs, num_iters))\n",
        "  for j in range(0, num_runs):\n",
        "    losses, W1, W2, x, y = run_gd(n0, n1, n2, m, sigma, num_iters)\n",
        "\n",
        "    # find hessians of W1, W2\n",
        "    H0, H1, H = hessian(W1, W2, x, y, losses[-1], losses[0])\n",
        "    eigs_H0, eigs_H1, eigs_H = npla.eigvalsh(H0), npla.eigvalsh(H1), npla.eigvalsh(H)\n",
        "\n",
        "    H0_eigs_arr = np.concatenate((H0_eigs_arr, eigs_H0), axis=None)\n",
        "    H1_eigs_arr = np.concatenate((H1_eigs_arr, eigs_H1), axis=None)\n",
        "    H_eigs_arr = np.concatenate((H_eigs_arr, eigs_H), axis=None)\n",
        "    \n",
        "    loss_array[j] = losses\n",
        "  return H0_eigs_arr, H1_eigs_arr, H_eigs_arr, loss_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqHdSrtMOpsP"
      },
      "source": [
        "## Plot Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DVe-OwhGAaO"
      },
      "source": [
        "def plot_losses(losses, phi, num_iters, num_runs):\n",
        "  mean_losses = np.average(losses, axis = 0)\n",
        "  std_losses = np.std(losses, axis=0)\n",
        "  ci = 1.96 * std_losses / np.sqrt(num_runs)\n",
        "  \n",
        "  plt.yscale('log')\n",
        "  plt.title('Loss Function throughout Gradient Descent')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "\n",
        "  plt.plot(np.arange(1, len(mean_losses) + 1), mean_losses, label='$\\phi$ = '+ str(round(phi, 1)))\n",
        "  plt.fill_between(np.arange(1, len(mean_losses) + 1), mean_losses + ci, mean_losses - ci, alpha=.2)\n",
        "  plt.annotate(mean_losses[num_iters - 1], (num_iters - 1, mean_losses[num_iters - 1]))\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwxqRSod93oZ"
      },
      "source": [
        "def plot_eigs(H0_eigs, H1_eigs, H_eigs):\n",
        "  plt.title(\"H0's Eigenvalues\")\n",
        "  plt.ylabel('Density')\n",
        "  plt.xlabel('Eigenvalue')\n",
        "  plt.hist(H0_eigs[(abs(H0_eigs) < 100) & (abs(H0_eigs) > 0.01)], bins=50, density=True, ec='black')\n",
        "  plt.show()\n",
        "  \n",
        "  plt.title(\"H1's Eigenvalues\")\n",
        "  plt.ylabel('Density')\n",
        "  plt.xlabel('Eigenvalue')\n",
        "  plt.hist(H1_eigs[(abs(H1_eigs) < 100) & (abs(H1_eigs) > 0.0001)], bins=50, density=True, ec='black')\n",
        "  plt.show()\n",
        "\n",
        "  plt.title(\"H's Eigenvalues\")\n",
        "  plt.ylabel('Density')\n",
        "  plt.xlabel('Eigenvalue')\n",
        "  plt.hist(H_eigs[(abs(H_eigs) < 100) & (abs(H_eigs) > 0.0001)], bins=50, density=True, ec='black')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2laXbryLr2L"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCi0k8N0xHrH"
      },
      "source": [
        "n0_widget = widgets.SelectionSlider(options=[10, 20, 50], \n",
        "                             value=20, \n",
        "                             description='n0', \n",
        "                             disabled=False, \n",
        "                             continuous_update=True, \n",
        "                             orientation='horizontal', \n",
        "                             readout=True)\n",
        "n1_widget = widgets.SelectionSlider(options=[10, 20, 50], \n",
        "                             value=20, \n",
        "                             description='n1', \n",
        "                             disabled=False, \n",
        "                             continuous_update=True, \n",
        "                             orientation='horizontal', \n",
        "                             readout=True)\n",
        "n2_widget = widgets.SelectionSlider(options=[10, 20, 50], \n",
        "                             value=20, \n",
        "                             description='n2', \n",
        "                             disabled=False, \n",
        "                             continuous_update=True, \n",
        "                             orientation='horizontal', \n",
        "                             readout=True)\n",
        "m_widget = widgets.SelectionSlider(options=[10, 50, 200], \n",
        "                             value=50, \n",
        "                             description='m', \n",
        "                             disabled=False, \n",
        "                             continuous_update=True, \n",
        "                             orientation='horizontal', \n",
        "                             readout=True)\n",
        "num_iters_widget = widgets.SelectionSlider(options=[0, 10, 100, 1000, 2500, 5000, 10000], \n",
        "                             value=2500, \n",
        "                             description='num_iters', \n",
        "                             disabled=False, \n",
        "                             continuous_update=True, \n",
        "                             orientation='horizontal', \n",
        "                             readout=True)\n",
        "\n",
        "def plot_it(n0, n1, n2, m, num_iters):\n",
        "  string = f'n0 = {n0}, n1 = {n1}, n2 = {n2}, m = {m}, num_iters = {num_iters}'\n",
        "  print('updating the plot with ' + string)\n",
        "\n",
        "  sigma = 1\n",
        "  num_runs = 2\n",
        "  H0_eigs, H1_eigs, H_eigs, losses = run_sim(n0, n1, n2, m, sigma, num_runs)\n",
        "\n",
        "  phi = ((n0 + n2) * n1) / (n2 * m)\n",
        "  plot_losses(losses, phi, num_iters, num_runs)\n",
        "  plot_eigs(H0_eigs, H1_eigs, H_eigs)\n",
        "\n",
        "interactive(plot_it, n0=n0_widget, n1=n1_widget, n2=n2_widget, m=m_widget, num_iters=num_iters_widget)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}